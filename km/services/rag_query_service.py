"""
RAG Query Service for km-for-agent-builder
æ•´åˆäº† km-for-agent-builder-client çš„æŸ¥è©¢åŠŸèƒ½
"""
import os
import json
from pathlib import Path
from types import SimpleNamespace
from typing import Dict, List, Optional
from loguru import logger
import sys
parent_dir = os.path.dirname(os.path.dirname(__file__))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)
from config import settings, get_user_prompt_template
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from services.bm25_index_manager import BM25IndexManager

# ç‚ºäº†å‘å¾Œç›¸å®¹ï¼Œæª¢æŸ¥ BM25 æ˜¯å¦å¯ç”¨ï¼ˆç”± BM25IndexManager è™•ç†å¯¦éš›åŠŸèƒ½ï¼‰
try:
    from rank_bm25 import BM25Okapi  # noqa: F401
    import jieba  # noqa: F401
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    logger.warning("BM25 dependencies not available. Install with: pip install rank-bm25 jieba")


class RAGQueryService:
    """RAG æŸ¥è©¢æœå‹™"""
    
    def __init__(self, embedding_model: Optional[HuggingFaceEmbeddings] = None):
        self.embedding_model = embedding_model
        self.current_model_path = None
        self.bm25_manager = BM25IndexManager()  # BM25 ç´¢å¼•ç®¡ç†å™¨
        # å„ªå…ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼Œå¦‚æœç‚ºç©ºå†ä½¿ç”¨ settings è¨­å®š
        env_search_algorithm = os.getenv('SEARCH_ALGORITHM', '').strip()
        if env_search_algorithm:
            self.search_algorithm = env_search_algorithm.lower()
            logger.info(f"Using search algorithm from environment variable: {self.search_algorithm}")
        else:
            self.search_algorithm = settings.SEARCH_ALGORITHM.lower()
            logger.info(f"Using search algorithm from settings: {self.search_algorithm}")
        
        # é©—è­‰æœå°‹æ¼”ç®—æ³•è¨­å®š
        if self.search_algorithm not in ['semantic', 'bm25']:
            logger.warning(f"Invalid search algorithm '{self.search_algorithm}', defaulting to 'semantic'")
            self.search_algorithm = 'semantic'
        
        if self.search_algorithm == 'bm25' and not BM25_AVAILABLE:
            logger.warning("BM25 requested but not available, falling back to semantic search")
            self.search_algorithm = 'semantic'
        
        if embedding_model == None:
            self.search_algorithm = 'bm25'
    
    def _get_collection(self, collection_name: str, chroma_path: str = None):
        """ç²å–æˆ–å‰µå»º Chroma collection - æ¯æ¬¡éƒ½é‡æ–°è®€å–"""

        chroma_path = os.path.join(settings.BASE_FOLDER, collection_name, "chroma_db")
        
        # æ¯æ¬¡éƒ½é‡æ–°å‰µå»º collectionï¼Œä¸ä½¿ç”¨ç·©å­˜
        logger.info(f"Loading collection: {collection_name} from path: {chroma_path}")
        
        try:
            collection = Chroma(
                persist_directory=chroma_path,
                embedding_function=self.embedding_model,
                collection_name=collection_name
            )
          
            logger.info(f"Successfully loaded collection: {collection_name}")
            return collection
        except Exception as e:
            logger.error(f"Failed to load collection {collection_name} from {chroma_path}: {str(e)}")
            raise e
    
    def clear_bm25_cache(self, collection_name: str = None):
        """æ¸…é™¤ BM25 ç·©å­˜"""
        self.bm25_manager.clear_cache(collection_name)
    
    def get_available_collections(self) -> List[str]:
        """
        ç²å–å¯ç”¨çš„ collection åˆ—è¡¨
        
        æƒæ BASE_FOLDER ä¸‹çš„æ‰€æœ‰ç›®éŒ„ï¼Œæ‰¾å‡ºåŒ…å« chroma_db å­ç›®éŒ„çš„ç›®éŒ„ä½œç‚ºå¯ç”¨çš„ collections
        
        Returns:
            å¯ç”¨çš„ collection åç¨±åˆ—è¡¨
        """
        collections = []
        base_folder = Path(settings.BASE_FOLDER)
        
        if not base_folder.exists():
            logger.warning(f"Base folder does not exist: {base_folder}")
            return collections
        
        # æƒæ base_folder ä¸‹çš„æ‰€æœ‰ç›®éŒ„
        for item in base_folder.iterdir():
            if item.is_dir():
                # æª¢æŸ¥æ˜¯å¦æœ‰ chroma_db å­ç›®éŒ„
                chroma_db_path = item / "chroma_db"
                if chroma_db_path.exists() and chroma_db_path.is_dir():
                    collections.append(item.name)
                    logger.debug(f"Found collection: {item.name}")
        
        logger.info(f"Found {len(collections)} available collections: {collections}")
        return sorted(collections)
    
    def get_rag_context_with_file_content(self, chroma: Chroma, collection_name: str, question: str, 
                                          k: int = 5, search_algorithm: Optional[str] = None, language: str = "zh-TW") -> Dict:
        """
        æ ¹æ“šå•é¡Œå¾ chroma æª¢ç´¢ç›¸é—œå…§å®¹ï¼Œä¸¦å¾å°æ‡‰çš„ merged file ä¸­è®€å–å®Œæ•´å…§å®¹ä¾†æ§‹å»ºèŠå¤©æ¶ˆæ¯
        
        Args:
            collection_name: é›†åˆåç¨±
            question: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„ top-k æ•¸é‡
        
        Returns:
            dict: {
                'filename': str,  # é¸ä¸­çš„æ–‡ä»¶å
                'chat_messages': List[dict],  # æ¨ç†çš„èŠå¤©æ¶ˆæ¯åˆ—è¡¨
                'merged_content': str,  # åˆä½µçš„å…§å®¹
                'error': str  # éŒ¯èª¤ä¿¡æ¯ï¼ŒæˆåŠŸæ™‚ç‚ºç©ºå­—ç¬¦ä¸²
            }
        """
        try:
            # æ ¹æ“šè¨­å®šçš„æ¼”ç®—æ³•é€²è¡Œæœå°‹ï¼ˆå…è¨±è¦†å¯«ï¼‰
            algo = (search_algorithm or self.search_algorithm or 'semantic').lower()
            if algo not in ['semantic', 'bm25']:
                logger.warning(f"Invalid search algorithm '{algo}', defaulting to 'semantic'")
                algo = 'semantic'
            if algo == 'bm25' and not BM25_AVAILABLE:
                logger.warning("BM25 requested but not available, falling back to semantic search")
                algo = 'semantic'
            
            # å¦‚æœä½¿ç”¨ semantic ä½†æ²’æœ‰ embedding_modelï¼Œè‡ªå‹•åˆ‡æ›åˆ° bm25
            if algo == 'semantic' and self.embedding_model is None:
                logger.warning("Semantic search requested but embedding_model is None, falling back to BM25")
                algo = 'bm25'

            logger.info(f"Searching for: '{question}' with k={k} using {algo} algorithm")
            
            if algo == 'bm25':
                # ä½¿ç”¨ BM25 æœå°‹
                bm25_results = self.bm25_manager.search(chroma, collection_name, question, k, language)
                if not bm25_results:
                    raise ValueError('no BM25 search results found')
                self._show_bm25_results(bm25_results)
                # è½‰æ› BM25 çµæœæ ¼å¼ä»¥åŒ¹é…èªæ„æœå°‹çš„æ ¼å¼
                # ä½¿ç”¨ SimpleNamespace å‰µå»º Document-like å°è±¡ä¾†çµ±ä¸€ä»‹é¢
                results = []
                for result in bm25_results:
                    # å‰µå»ºé¡ä¼¼ Document çš„å°è±¡
                    doc = SimpleNamespace(
                        page_content=result['content'],
                        metadata=result['metadata']
                    )
                    # å°‡ BM25 åˆ†æ•¸è½‰æ›ç‚ºè·é›¢å½¢å¼ï¼ˆè² åˆ†æ•¸ï¼šåˆ†æ•¸è¶Šé«˜ï¼Œè² åˆ†æ•¸è¶Šå°ï¼Œèˆ‡èªæ„æœå°‹é‚è¼¯ä¸€è‡´ï¼‰
                    # èªæ„æœå°‹ï¼šåˆ†æ•¸è¶Šä½è¶Šå¥½ï¼ˆè·é›¢è¶Šå°ï¼‰
                    # BM25ï¼šåˆ†æ•¸è¶Šé«˜è¶Šå¥½ï¼ˆç›¸é—œæ€§è¶Šé«˜ï¼‰
                    # è½‰æ›ï¼šä½¿ç”¨è² åˆ†æ•¸è®“ BM25 åˆ†æ•¸è¶Šé«˜å°æ‡‰è·é›¢è¶Šå°
                    normalized_score = -result['score']
                    results.append((doc, normalized_score))
                
            else:
                # ä½¿ç”¨èªæ„æœå°‹ï¼ˆé»˜èªï¼‰
                results = chroma.similarity_search_with_score(question, k=k)
            
            logger.info(f"Search returned {len(results)} results")
            
            if not results:
                raise ValueError('no search results found')

            # é¡¯ç¤ºæœå°‹çµæœ
            self._show_search_results(results, max_chunk_length=150)

            # ç›´æ¥é¸å–ç¬¬ä¸€ç­†è³‡æ–™çš„ group_id
            first_doc, first_score = results[0]
            selected_group_id = first_doc.metadata.get('group_id', '')
            if not selected_group_id:
                raise ValueError('no valid group_id found in first result')
            
            # æ”¶é›†æ‰€æœ‰ chunks
            all_chunks = [doc.page_content for doc, score in results]
            
            logger.info(f"Selected group_id: {selected_group_id}")
            
            source_filename = selected_group_id
            logger.info(f"Selected source filename: {source_filename}")
            merge_file_name = f"{source_filename}.txt"
            logger.info(f"Selected merge filename: {merge_file_name}")
            
            merged_file_folder = os.path.join(settings.BASE_FOLDER, collection_name, "merged_files")

            # æ§‹å»ºå®Œæ•´çš„ merged file è·¯å¾‘
            merged_file_path = os.path.join(
                settings.BASE_FOLDER,
                collection_name,
                "merged_files",
                merge_file_name
            )
            
            # å¾æŒ‡å®šçš„ txt æª”æ¡ˆä¸­è®€å–å…§å®¹ä½œç‚º chunk
            merged_content = ""
            try:
                logger.info(f"Attempting to read merged file: {merged_file_path}")
                
                if os.path.exists(merged_file_path):
                    with open(merged_file_path, 'r', encoding='utf-8') as f:
                        merged_content = f.read().strip()
                    logger.info(f"Successfully read merged file, content length: {len(merged_content)} chars")
                else:
                    logger.warning(f"Merged file not found: {merged_file_path}")
                    return {
                        'filename': None,
                        'chat_messages': [],
                        'merged_content': '',
                        'error': 'merge file not found'
                    }
                    
            except Exception as file_error:
                logger.error(f"Failed to read merged file: {str(file_error)}")
                return {
                    'filename': None,
                    'chat_messages': [],
                    'merged_content': '',
                    'error': 'merge file not found'
                }
            
            # å‰µå»ºç”¨æ–¼æ¨ç†çš„èŠå¤©æ¶ˆæ¯
            chat_messages = []
            
            # å¦‚æœ system prompt ä¸ç‚ºç©ºï¼Œå‰‡æ·»åŠ  system æ¶ˆæ¯
            if settings.SYSTEM_PROMPT and settings.SYSTEM_PROMPT.strip():
                chat_messages.append({
                    "role": "system",
                    "content": settings.SYSTEM_PROMPT
                })
            
            # æ·»åŠ  user æ¶ˆæ¯
            chat_messages.append({
                "role": "system", 
                "content": get_user_prompt_template(km_lang=language, include_query=True).format(chunk=merged_content, query=question)
            })
            
            logger.info(f"Suggested merge file name: {merge_file_name}")
            logger.info(f"Generated {len(chat_messages)} chat messages")
            logger.debug(f"Retrieved {len(all_chunks)} document chunks")
            include_file= chroma.get(where={"group_id": selected_group_id})
            # logger.info(f'selected_group_id: {selected_group_id} {include_file}')
            include_file_list = []
            for file in include_file.get('metadatas'):
                source_file = file.get('source_file', '')
                if source_file and source_file not in include_file_list:
                    include_file_list.append(source_file)

            return {
                'filename': merge_file_name,
                'include_file_list': include_file_list,
                'chat_messages': chat_messages,
                'merged_content': merged_content,  # æ·»åŠ  merged_content å­—æ®µ
                'retrieved_chunks': all_chunks,  # æ–°å¢ï¼šæª¢ç´¢åˆ°çš„åŸå§‹ chunks
                'error': ''
            }
                
        except Exception as e:
            logger.error(f"get_rag_context_with_file_content error: {str(e)}")
            return {
                'filename': None,
                'chat_messages': [],
                'merged_content': '',
                'error': f'internal error: {str(e)}'
            }

    def _show_bm25_results(self, results: List[Dict]):
        simplified_results = []
        for r in results:
            md = r.get("metadata", {}) or {}
            simplified_results.append({
                "score": r.get("score", 0),
                "source_file": md.get("source_file"),
                "group_id": md.get("group_id"),
            })

        output = {
            "count": len(simplified_results),
            "results": simplified_results,
        }
        # ä½¿ç”¨ logger é¿å… Windows æ—¢å®šç·¨ç¢¼ï¼ˆcp950 ç­‰ï¼‰é€ æˆ UnicodeEncodeError
        logger.info(json.dumps(output, ensure_ascii=False, indent=2))
    
    def _show_search_results(self, results: List[tuple], max_chunk_length: int = 100):
        """
        é¡¯ç¤ºæœå°‹çµæœï¼ˆèªæ„æœå°‹æˆ– BM25ï¼‰
        
        Args:
            results: List[Tuple[doc_like, score]]ï¼Œdoc_like éœ€å…·æœ‰ metadata èˆ‡ page_content
            max_chunk_length: chunk å…§å®¹æœ€å¤§é¡¯ç¤ºé•·åº¦ï¼Œè¶…éæœƒæˆªæ–·
        """
        simplified_results = []
        for idx, (doc, score) in enumerate(results):
            chunk_content = doc.page_content
            # æˆªæ–·éé•·çš„ chunk å…§å®¹
            if len(chunk_content) > max_chunk_length:
                chunk_preview = chunk_content[:max_chunk_length] + "..."
            else:
                chunk_preview = chunk_content
            
            md = doc.metadata or {}
            simplified_results.append({
                "index": idx,
                "score": round(score, 4),
                "group_id": md.get("group_id", "N/A"),
                "source_file": md.get("source_file", "N/A"),
                # "chunk_preview": chunk_preview
            })
        
        output = {
            "count": len(simplified_results),
            "results": simplified_results
        }
        # ä½¿ç”¨ logger é¿å… Windows æ—¢å®šç·¨ç¢¼ï¼ˆcp950 ç­‰ï¼‰é€ æˆ UnicodeEncodeError
        logger.info("Search Results:")
        logger.info(json.dumps(output, ensure_ascii=False, indent=2))

    # def _select_group_id(self, results, algo: str):
    #     """
    #     çµ¦å®šæª¢ç´¢çµæœèˆ‡æ¼”ç®—æ³•ï¼Œé¸æ“‡åˆé©çš„ group_id ä¸¦å›å‚³ (selected_group_id, all_chunks)
    #     results: List[Tuple[doc_like, score]]ï¼Œdoc_like éœ€å…·æœ‰ metadata èˆ‡ page_content
    #     """
    #     if not results:
    #         raise ValueError('no search results found')

    #     # BM25ï¼šç›´æ¥é¸ç¬¬ä¸€ç­† group
    #     if algo == 'bm25':
    #         first_group_id = results[0][0].metadata.get('group_id', '')
    #         if not first_group_id:
    #             raise ValueError('no valid group_ids found')
    #         return first_group_id, [results[0][0].page_content]

    #     # Semanticï¼šä»¥æ¬¡æ•¸æœ€å¤šï¼Œè‹¥ä¸¦åˆ—å‰‡å– similarity_sum è¼ƒå°è€…
    #     group_stats = {}
    #     all_chunks = []
    #     for doc, score in results:
    #         group_id = doc.metadata.get('group_id', '')
    #         chunk_content = doc.page_content
    #         all_chunks.append(chunk_content)
    #         if group_id:
    #             if group_id not in group_stats:
    #                 group_stats[group_id] = {
    #                     'count': 0,
    #                     'similarity_sum': 0.0,
    #                     'scores': [],
    #                     'chunks': []
    #                 }
    #             group_stats[group_id]['count'] += 1
    #             group_stats[group_id]['similarity_sum'] += score
    #             group_stats[group_id]['scores'].append(score)
    #             group_stats[group_id]['chunks'].append(chunk_content)
    #             logger.info(f"group_id: {group_id}, similarity_sum: {group_stats[group_id]['similarity_sum']}, scores: {group_stats[group_id]['scores']}")

    #     # logger.info(f"group_stats: {group_stats}")
    #     if not group_stats:
    #         raise ValueError('no valid group_ids found')

    #     max_count = max(stats['count'] for stats in group_stats.values())
    #     top_groups = [group_id for group_id, stats in group_stats.items() if stats['count'] == max_count]
    #     if len(top_groups) == 1:
    #         return top_groups[0], all_chunks

    #     best_group = None
    #     best_similarity_sum = float('inf')
    #     for group_id in top_groups:
    #         similarity_sum = group_stats[group_id]['similarity_sum']
    #         if similarity_sum < best_similarity_sum:
    #             best_similarity_sum = similarity_sum
    #             best_group = group_id
    #     return best_group, all_chunks

    def prepare_rag_messages(self, collection_name: str, query: str, k: int = 5, 
                            language: str = "zh-TW") -> Dict:
        """
        æº–å‚™ RAG æŸ¥è©¢æ‰€éœ€çš„ messages å’Œä¸Šä¸‹æ–‡è³‡è¨Šï¼ˆå–®ä¸€è·è²¬ï¼šåªè² è²¬ RAG æª¢ç´¢å’Œ messages æ§‹å»ºï¼‰
        
        Args:
            collection_name: é›†åˆåç¨±
            query: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„ top-k æ•¸é‡
            language: èªè¨€è¨­å®š
        
        Returns:
            dict: {
                'success': bool,
                'messages': List[Dict],  # OpenAI æ ¼å¼çš„ messages
                'message': str,
                'merged_file': str,
                'source_files': List[str],
                'retrieved_chunks': List[str],
                'merged_content': str,
                'debug_info': Dict  # èª¿è©¦è³‡è¨Š
            }
        """
        try:
            # ç²å– collection
            chroma = self._get_collection(collection_name)
            
            # ç²å– RAG ä¸Šä¸‹æ–‡
            result = self.get_rag_context_with_file_content(
                chroma, collection_name, query, k, 
                search_algorithm=self.search_algorithm, 
                language=language
            )
            
            if not result.get("success", True) or result.get("error"):
                return {
                    'success': False,
                    'messages': [],
                    'message': result.get("error", "Failed to get RAG context"),
                    'merged_file': None,
                    'source_files': None,
                    'retrieved_chunks': None,
                    'merged_content': None,
                    'debug_info': {}
                }
            
            # æå–æ–‡ä»¶è³‡è¨Š
            filename = result.get("filename", "")
            filename_wo_ext = os.path.splitext(filename)[0] if filename else ""
            merged_content = result.get("merged_content", "")
            
            # æ§‹å»º messages
            messages = []
            
            # æ·»åŠ  system messageï¼ˆå¦‚æœæœ‰ï¼‰
            if settings.SYSTEM_PROMPT and settings.SYSTEM_PROMPT.strip():
                messages.append({
                    "role": "system",
                    "content": settings.SYSTEM_PROMPT
                })
            
            # æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯
            user_prompt_template = get_user_prompt_template(km_lang=language, include_query=True)
            user_content = user_prompt_template.format(
                chunk=merged_content,
                query=query
            )
            messages.append({
                "role": "system",
                "content": user_content
            })
            
            # æ§‹å»ºèª¿è©¦è³‡è¨Š
            debug_info = {
                "km_service_used": True,
                "collection": collection_name,
                "filename": filename_wo_ext,
                "original_query": query,
                "rag_content_length": len(merged_content),
                "include_file_list": result.get("include_file_list", [])
            }
            
            return {
                'success': True,
                'messages': messages,
                'message': 'RAG messages prepared successfully',
                'merged_file': filename,
                'source_files': result.get("include_file_list", []),
                'retrieved_chunks': result.get("retrieved_chunks", []),
                'merged_content': merged_content,
                'debug_info': debug_info
            }
            
        except Exception as e:
            logger.error(f"Error preparing RAG messages: {e}")
            return {
                'success': False,
                'messages': [],
                'message': f"Internal error: {str(e)}",
                'merged_file': None,
                'source_files': None,
                'retrieved_chunks': None,
                'merged_content': None,
                'debug_info': {}
            }

    def generate_openai_payload(self, collection_name: str, query: str, k: int = 5, 
                               stream: bool = True, model: str = "gpt-4", 
                               params: Optional[Dict] = None, language: str = "zh-TW") -> Dict:
        """
        ç”Ÿæˆæ¨™æº– OpenAI æ ¼å¼çš„ payloadï¼ˆå–®ä¸€è·è²¬ï¼šåªè² è²¬çµ„è£å’Œåºåˆ—åŒ– payloadï¼‰
        
        Args:
            collection_name: é›†åˆåç¨±
            query: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„ top-k æ•¸é‡
            stream: æ˜¯å¦æµå¼è¼¸å‡º
            model: æ¨¡å‹åç¨±
            params: é¡å¤–åƒæ•¸
            language: èªè¨€è¨­å®š
        
        Returns:
            dict: {
                'success': bool,
                'payload_raw': str,  # JSON æ ¼å¼çš„ payload å­—ç¬¦ä¸²
                'message': str,
                'merged_file': str,
                'source_files': List[str],
                'retrieved_chunks': List[str],
                'merged_content': str
            }
        """
        try:
            # ä½¿ç”¨æ–°çš„ prepare_rag_messages å‡½æ•¸
            rag_result = self.prepare_rag_messages(collection_name, query, k, language)
            
            if not rag_result['success']:
                return {
                    'success': False,
                    'payload_raw': '',
                    'message': rag_result['message'],
                    'merged_file': None,
                    'source_files': None,
                    'retrieved_chunks': None,
                    'merged_content': None
                }
            
            # æ§‹å»ºå®Œæ•´çš„ payload å°è±¡
            payload_obj = {
                "stream": stream,
                "model": model,
                "messages": rag_result['messages'],
                "max_tokens": params.get("max_tokens", 2048) if params else 2048,
                "temperature": params.get("temperature", 0.7) if params else 0.7,
                "top_p": params.get("top_p", 1.0) if params else 1.0,
                # "debug_llm_payload": rag_result['debug_info']
            }
            
            # åºåˆ—åŒ–ç‚º JSON å­—ç¬¦ä¸²
            payload_raw = json.dumps(payload_obj, ensure_ascii=False)
            
            return {
                'success': True,
                'payload_raw': payload_raw,
                'message': 'OpenAI payload generated successfully',
                'merged_file': rag_result['merged_file'],
                'source_files': rag_result['source_files'],
                'retrieved_chunks': rag_result['retrieved_chunks'],
                'merged_content': rag_result['merged_content']
            }
            
        except Exception as e:
            logger.error(f"Error generating OpenAI payload: {e}")
            return {
                'success': False,
                'payload_raw': '',
                'message': f"Internal error: {str(e)}",
                'merged_file': None,
                'source_files': None,
                'retrieved_chunks': None,
                'merged_content': None
            }

if __name__ == '__main__':
    # åœ¨æ¸¬è©¦æ¨¡å¼ä¸‹ä½¿ç”¨ 64 ç¶­çš„å‡åµŒå…¥æ¨¡å‹
    class TestFakeEmbeddings:
        def __init__(self, *args, **kwargs):
            pass

        def embed_documents(self, texts):
            # Deterministic pseudo-embeddings based on text length
            import numpy as np
            rng = np.random.default_rng(42)
            vectors = []
            for t in texts:
                length = max(1, len(t))
                rng_local = np.random.default_rng(length)
                vec = rng_local.normal(size=64)  # 64 ç¶­ï¼Œèˆ‡æ¸¬è©¦è…³æœ¬ä¸€è‡´
                # L2 normalize
                norm = (vec**2).sum() ** 0.5
                if norm != 0:
                    vec = vec / norm
                vectors.append(vec.tolist())
            return vectors

        def embed_query(self, text):
            return self.embed_documents([text])[0]

    # å‰µå»º RAG æŸ¥è©¢æœå‹™ä¸¦æ›¿æ›åµŒå…¥æ¨¡å‹
    rag_query_service = RAGQueryService()
    rag_query_service.embedding_model = TestFakeEmbeddings()
    logger.info(f"ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šä½¿ç”¨ 64 ç¶­å‡åµŒå…¥æ¨¡å‹ï¼Œæœå°‹æ¼”ç®—æ³•ï¼š{rag_query_service.search_algorithm.upper()}")

    collections = rag_query_service.get_available_collections()
    logger.info(f"Available collections: {collections}")
    
    # ç°¡å–®çš„ RAG æŸ¥è©¢æ¸¬è©¦
    if collections:
        test_collection = collections[0]
        logger.info(f"\næ¸¬è©¦ RAG æŸ¥è©¢ - Collection: {test_collection}")
        
        # # å…ˆæª¢æŸ¥ collection ç‹€æ…‹
        # try:
        #     chroma = rag_query_service._get_collection(test_collection)
        #     count = chroma._collection.count()
        #     logger.info(f"Collection æ–‡æª”æ•¸é‡: {count}")
        # except Exception as e:
        #     logger.info(f"âš ï¸  ç„¡æ³•ç²å– collection ç‹€æ…‹: {str(e)}")
        
        result = rag_query_service.get_rag_context_with_file_content(
            collection_name=test_collection,
            question="what is NVM ExpressTM",
            k=3
        )
        
        if result.get('error'):
            logger.info(f"âŒ æŸ¥è©¢å¤±æ•—: {result['error']}")
        else:
            logger.info(f"âœ… æŸ¥è©¢æˆåŠŸ")
            logger.info(f"   æ¨è–¦æ–‡ä»¶: {result.get('filename', 'N/A')}")
            logger.info(f"   æ¶ˆæ¯æ•¸é‡: {len(result.get('chat_messages', []))}")
            # logger.info(result)
        
        # æ¸¬è©¦ generate_openai_payload åŠŸèƒ½
        logger.info(f"\n=== æ¸¬è©¦ OpenAI Payload ç”Ÿæˆ ===")
        try:
            openai_result = rag_query_service.generate_openai_payload(
                collection_name=test_collection,
                query="what is NVM ExpressTM",
                k=3,
                stream=False,
                model="gpt-3.5-turbo",
                params={"temperature": 0.7, "max_tokens": 1000}
            )
            
            if openai_result['success']:
                logger.info(f"âœ… OpenAI Payload ç”ŸæˆæˆåŠŸ")
                logger.info(f"   æ¶ˆæ¯: {openai_result['message']}")
                logger.info(f"   Payload é•·åº¦: {len(openai_result['payload_raw'])} å­—ç¬¦")
                
                # é¡¯ç¤º payload å…§å®¹ï¼ˆå‰ 500 å­—ç¬¦ï¼‰
                payload_preview = openai_result['payload_raw'][:500]
                logger.info(f"   Payload é è¦½: {payload_preview}...")
                
                # å˜—è©¦è§£æ JSON ä¾†é©—è­‰æ ¼å¼
                try:
                    import json
                    payload_obj = json.loads(openai_result['payload_raw'])
                    logger.info(f"   âœ… JSON æ ¼å¼é©—è­‰é€šé")
                    logger.info(f"   æ¨¡å‹: {payload_obj.get('model', 'N/A')}")
                    logger.info(f"   æµå¼: {payload_obj.get('stream', 'N/A')}")
                    logger.info(f"   æ¶ˆæ¯æ•¸é‡: {len(payload_obj.get('messages', []))}")
                except json.JSONDecodeError as e:
                    logger.info(f"   âŒ JSON æ ¼å¼éŒ¯èª¤: {str(e)}")
            else:
                logger.info(f"âŒ OpenAI Payload ç”Ÿæˆå¤±æ•—: {openai_result['message']}")
                
        except Exception as e:
            logger.info(f"âŒ OpenAI Payload æ¸¬è©¦å¤±æ•—: {str(e)}")
    else:
        logger.info("\nâš ï¸  æ²’æœ‰å¯ç”¨çš„ collections")

